# thermo-adaptive-pipeline
An eco-friendly pipeline for fine-tuning and inferencing transformer-based language models engineered to actively prevent hardware overheating.

## 1. Introduction 

Brute-force scaling of hardware-based higher computational intensity results in greater power consumption and heat generation, impacting battery life and potentially requiring more sophisticated cooling solutions. [1](https://arxiv.org/html/2501.14757v1) [2](https://www.eetimes.com/the-impact-of-the-end-of-moores-law-on-the-ai-gold-rush/) [3](http://www.recoverit.20m.com/whats_new_1.html)

According to the laws of thermodynamics, power consumed eventually turns into heat. Without adequate thermal management, excess heat can damage components, reduce reliability, and throttle performance. [4](https://adaptivesupport.amd.com/s/question/0D52E00006hpLoOSAU/thermal-efficiency-for-ultra-scale-fpga?language=en_US] )
[5](https://www.sparkl.me/learn/collegeboard-ap/physics-c-electricity-and-magnetism/power-dissipation-in-circuits/revision-notes/723) [6](https://www.windings.com/post/overcoming-technical-challenges-in-high-temperature-motor-environments/) [7](https://cvgstrategy.com/wp-content/uploads/2019/08/MIL-STD-810H-Method-501.7-High-Temperature.pdf) [8](https://www.monolithicpower.com/en/learning/mpscholar/automotive-electronics/emc-management-in-auto-electronics/need-for-thermal-management) [9](https://energy.sustainability-directory.com/term/advanced-heat-dissipation/) [10](https://www.youtube.com/watch?v=dFfsOChfTag) [11](https://en.wikipedia.org/wiki/First_law_of_thermodynamics) [12](https://ijisrt.com/wp-content/uploads/2017/03/Perpetual-Motion-with-Solar-and-Wind-Energy-Hybrid-System-2.pdf) [13](https://link.springer.com/rwe/10.1007/978-3-319-95864-4_35)

Without effective thermal management (cooling systems), the accumulation of this waste heat leads to several critical problems. [14](https://energy.sustainability-directory.com/term/advanced-heat-dissipation/)

Electronic components are designed to operate within specific temperature ranges. Consistently running at the upper limits significantly accelerates wear and tear, drastically shortening the device's operational life. [15](https://www.researchgate.net/publication/271553806_The_effect_of_temperature_on_the_reliability_of_electronic_components) [16](https://www.monolithicpower.com/en/learning/mpscholar/automotive-electronics/emc-management-in-auto-electronics/need-for-thermal-management) [17](https://cvgstrategy.com/wp-content/uploads/2019/08/MIL-STD-810H-Method-501.7-High-Temperature.pdf)

Modern processors and electronic systems have built-in thermal protection mechanisms. When a critical temperature threshold is reached, the system intentionally slows down (throttles) its performance to reduce power consumption and generate less heat, preventing immediate damage. [18](https://cvgstrategy.com/wp-content/uploads/2019/08/MIL-STD-810H-Method-501.7-High-Temperature.pdf) [19](https://www.sciencedirect.com/science/article/abs/pii/S235271022301402X) [20](https://www.windings.com/post/overcoming-technical-challenges-in-high-temperature-motor-environments/) [21](https://www.mdpi.com/2076-3417/15/16/9099) 
[22](https://www.monolithicpower.com/en/learning/mpscholar/automotive-electronics/emc-management-in-auto-electronics/need-for-thermal-management#:~:text=Deleterious%20Effects%20on%20Materials:%20The%20rise%20in,For%20example%2C%20semiconductors%20are%20extremely%20temperature%20sensitive)

High-efficiency data centers rely on systems like cooling towers or chillers to reject the massive amount of heat generated by the GPUs.

To maintain the servers within optimal temperature ranges for performance and longevity, the cooling must be highly effective and robust.

The high power draw of high-precision LLM clusters creates a tremendous thermal load on the data center infrastructure.

The amount of electrical energy the system draws per unit of time (measured in Watts). More active components and faster switching speeds inherently draw more power.

As a significant portion of the consumed electrical power is dissipated as waste heat (due to electrical resistance and leakage currents in transistors), the faster the switching and the denser the circuitry, the more concentrated this heat becomes.

While power efficiency has increased dramatically on a "per-operation" basis compared to older architectures, the total heat generated by the newest chips is simultaneously skyrocketing due to the sheer exponential increase in computational demand and density.

The total cooling requirements have therefore not diminished; they have instead increased to unprecedented levels, requiring a revolutionary shift in cooling technology.

As chips get larger and more complex, memory bandwidth requirements explode, becoming a the dominant source of heat, 
contributing to the scale of overall Power Usage Effectiveness (PUE) of a data center.

The efficiency gains have been overwhelmed by the demand for exponentially more computation, leading to a critical thermal management crisis that demands radically new cooling solutions at the software, hardware and social levels.

While the mainstream discourse often focuses on energy usage and carbon footprint of the computing sector at a global scale, the local socio-environmental consequences—such as health impacts, water usage, noise pollution, infrastructural strain, and economic burden—remain largely underexplored and poorly addressed. [23](https://arxiv.org/html/2506.03367v1)

## 2. Water Usage and its emissions

Freshwater scarcity is a global problem that requires collective efforts across all industry sectors. [24](https://arxiv.org/html/2405.17469v1) [25](https://andthewest.stanford.edu/2025/thirsty-for-power-and-water-ai-crunching-data-centers-sprout-across-the-west/)

Some data centers use water-based cooling systems, which can lead to substantial water consumption. An average Google data center consumes approximately 450,000 gallons of water per day. This can strain local water resources, especially in areas prone to drought or with limited water supply. Overall, data center cooling systems are responsible for over 40% of their electricity usage. [26](https://www.staxengineering.com/stax-hub/the-environmental-impact-of-data-centers/) [27](https://arxiv.org/html/2506.03367v1)

These facilities must operate 24/7 to prevent downtime, they rely many time on fossil fuel, like diesel generators as a failsafe during power disruptions. [28](https://www.staxengineering.com/stax-hub/the-environmental-impact-of-data-centers/) 

These diesel generators can be massive, ranging in size from 1.5 MW to over 3 MW each. Most generators are designed to provide 1.5-2 times the total connected load. These generators emit significant amounts of particulate matter (PM), nitrogen oxides (NOx), sulfur dioxide (SO₂), and carbon dioxide (CO₂) contribute to exacerbating the problematic current state climate change, and pose serious health risks to nearby communities and overall worlwide interconnected natural systems. [29](https://www.staxengineering.com/stax-hub/the-environmental-impact-of-data-centers/)

Data centers are not a renewable resource. The average lifespan of a data center is approximately 10-15 years and needs continuous maintenance just like a gas-powered vehicle. While the initial construction of a data center generates jobs, after its completion, the number of employees needed at the center drops by approximately 90%. So it's not only bad but also short-life-spanned, at least with current notions of datacenter design. [30](https://utulsa.edu/news/data-centers-draining-resources-in-water-stressed-communities/) [31](https://www.reccessary.com/en/news/data-centers-consume-massive-amounts-of-water)

The massive water use for cooling (evaporation in cooling towers) can strain local water resources, especially in drought-prone areas. All major ai companies have reported significant increases in water use driven by AI. 

Some cooling systems use potent fluorinated greenhouse gases (F-GHGs) as refrigerants. If these gases leak, and they eventually do, they have a global warming potential thousands of times greater than CO₂.

So, The overall environmental impact includes not only the water consumed, but also the energy used for cooling, and the water required for the power plants that supply electricity to the data centers.  [32](https://www.bloomberg.com/graphics/2025-ai-impacts-data-centers-water-data/) [33](https://www.youtube.com/watch?v=HfRw-nV6b8M&t) [34](https://www.youtube.com/watch?v=cl1ctf1_JxE&t/) [35](https://www.reccessary.com/en/news/data-centers-consume-massive-amounts-of-water)

Prioritizing one aspect of sustainability, such as reducing carbon emissions, while neglecting another crucial resource like water, creates an illusion of sustainability. [36](https://utulsa.edu/news/data-centers-draining-resources-in-water-stressed-communities/) [37](https://www.extremetech.com/computing/data-center-ai-gpus-may-have-extremely-short-lifespans) [38](https://www.jll.com/en-us/insights/why-data-centers-could-hit-obsolescence-sooner-than-you-think) [39](https://www.tomshardware.com/pc-components/gpus/datacenter-gpu-service-life-can-be-surprisingly-short-only-one-to-three-years-is-expected-according-to-unnamed-google-architect) [40](https://www.bcg.com/publications/2025/breaking-barriers-data-center-growth) [41](https://akfpartners.com/growth-blog/data-center-lifespan-risk)

It's actually pretty clear that we need holistic sustainability approaches that, inaddition to the emissions, also explicitly incorporate waterfootprint mitigation into its management; We need an eco-friendly, sustainable version of AI datacenters and energy power plants. [42](https://www.researchgate.net/publication/384349477_Water-Wise_Computing_Addressing_Data_Center_Water_Consumption_for_a_Sustainable_Future)


## 3. Emissions from Electricity Generation

The burning of fossil fuels for electricity, heat, and transportation
is the largest contributor to climate change, with global energy consumption accounting for over 73% of total greenhouse gas emissions. Power plants using coal, oil, and natural gas are the single largest source of these emissions.  [44](www.un.org/en/climatechange/science/causes-effects-climate-change) [45](https://www.wri.org/insights/4-charts-explain-greenhouse-gas-emissions-countries-and-sectors) [46](https://d-carbonize.eu/greenhouse-gases/comparison-ghg-emissions-sector/) [47](https://www.epa.gov/ghgemissions/global-greenhouse-gas-overview) [48](https://www.lse.ac.uk/granthaminstitute/explainers/what-direct-risks-does-ai-pose-to-the-climate-and-environment) [49](https://news.mit.edu/2025/explained-generative-ai-environmental-impact-0117) [50](https://www.iea.org/energy-system/buildings/)data-centres-and-data-transmission-networks [51](https://www.npr.org/2024/07/12/g-s1-9545/ai-brings-soaring-emissions-for-google-and-microsoft-a-major-contributor-to-climate-change)

The carbon dioxide (CO₂) and other greenhouse gases emitted by the power plants generating the electricity for the datacenters and ai datacenters. [52](https://www.climateimpact.com/news-insights/insights/carbon-footprint-of-ai/) [53](https://www.staxengineering.com/stax-hub/the-environmental-impact-of-data-centers/) [54](https://www.carbonbrief.org/ai-five-charts-that-put-data-centre-energy-use-and-emissions-into-context/) [55](https://www.sciencedirect.com/science/article/pii/S2589004224028645)

The energy sector, including natural gas operations, is a major source of anthropogenic methane emissions, primarily from fugitive leaks during extraction, processing, and transportation. Methane is the primary component of natural gas, and unburned gas leaking into the atmosphere is a potent greenhouse gas, with a warming effect over 80 times stronger than $CO_2$ over a 20-year period. AI Data centers use massive amounts of electricity. The environmental impact of this electricity consumption depends on the energy source used. If data centers are powered by natural gas-fired electricity, their indirect emissions include the upstream methane leaks from the gas supply chain. The data center itself primarily produces $CO_2$ from the combustion of the gas at the power plant, along with the release of other heat-trapping gases. [56](https://miq.org/thought-leadership/tech-firms-choosing-natural-gas-to-power-data-centers-should-ensure-it-has-been-third-party-certified-as-low-methane-emissions) [57](https://www.iea.org/reports/energy-and-ai/energy-supply-for-ai) [58](https://www.iea.org/energy-system/buildings/data-centres-and-data-transmission-networks) [59](https://www.unep.org/explore-topics/energy/facts-about-methane) [60](https://www.naturalgasworld.com/whats-up-with-methane-gas-in-transition-98516) [61](https://www.unep.org/explore-topics/energy/facts-about-methane) [62](https://www.epa.gov/ghgemissions/methane-emissions) [63](https://science.nasa.gov/earth/explore/earth-indicators/methane/) [64](https://www.ejosdr.com/download/harnessing-artificial-intelligence-for-methane-emissions-control-in-industrial-natural-gas-engines-17282.pdf)  [65](https://ember-energy.org/latest-insights/from-ai-to-emissions-aligning-asean-digital-growth-with-energy-transition/greening-data-centres/) [66](https://www.iea.org/reports/energy-and-ai/executive-summary)

## 4. Sulfur Dioxide ($SO_2$) Nitrogen Oxides ($NO_x$) and Particulate Matter (PM) Emissions

While Carbon Dioxide ($CO_2$) often dominates the conversation about AI's environmental impact, **Sulfur Dioxide ($SO_2$)**, **Nitrogen Oxides ($NO_x$)**, and **Particulate Matter (PM)** are critical pollutants associated with AI data centers. [67](https://netzeroinsights.com/resources/data-centers-environmental-cost/) [68](https://cap2i.eu/news/environmental-impact-of-data-centers-sustainable-solutions-5.php)  [69](https://blogs.lse.ac.uk/usappblog/2025/08/07/the-local-and-global-environmental-footprint-of-the-ai-driven-boom-in-data-centers) [70](https://blog.ucs.org/pablo-ortiz/what-are-the-environmental-impacts-of-artificial-intelligence) [71](https://www.socomec.co.uk/en-gb/solutions/business/data-centre/green-data-centres-balancing-performance-and-environmental-responsibility) [72](https://netzeroinsights.com/resources/data-centers-environmental-cost/) 

Unlike $CO_2$, which is a global greenhouse gas, these are **local air pollutants** that directly affect the health of communities nearby. They appear in AI data center emissions through two main channels: [73](https://ecology.wa.gov/air-climate/air-quality/data-centers) [74](https://lifestyle.sustainability-directory.com/learn/what-are-the-primary-air-pollutants-from-fossil-fuel-powered-data-centers/) [75](https://www.linkedin.com/pulse/environmental-impacts-ai-data-centers-conundrum-society-odutola-fbeoc/) [76](https://www.staxengineering.com/stax-hub/the-environmental-impact-of-data-centers/) [77](https://www.eea.europa.eu/mobile/data-and-maps/indicators/emissions-of-primary-particles-and-5/eea-32-sulphur-dioxide-so2-emissions-1) [78](https://envirodatagov.org/blogs/communities-close-to-epa-regulated-data-centers-face-heightened-air-pollution) [79](https://arxiv.org/abs/2509.21312) [80](https://www.epa.gov/power-sector/power-plants-and-neighboring-communities) [81](https://www.azocleantech.com/article.aspx?ArticleID=2042)

As previously mentioned, the most direct source of these pollutants from data centers is not the computers themselves, but the massive banks of diesel generators kept on-site. AI data centers require 100% uptime; if the grid fails, they must generate their own power immediately.

**Nitrogen Oxides ($NO_x$):** Diesel engines are notorious for producing high levels of $NO_x$. When data centers run these generators (for routine testing, maintenance, or during outages), they release concentrated plumes of $NO_x$. Reacts with other chemicals in the air to form smog and acid rain. It irritates human lungs and can exacerbate asthma.

**Particulate Matter (PM):** Diesel combustion produces "diesel soot," which consists of fine particles ($PM_{2.5}$). These microscopic particles can penetrate deep into the lungs and enter the bloodstream, causing cardiovascular and respiratory disease.
  
**Sulfur Dioxide ($SO_2$):** While modern "ultra-low sulfur diesel" has reduced this, older generators or those with lower fuel standards still emit $SO_2$. A precursor to acid rain and a severe respiratory irritant.

AI training clusters (like those used for LLMs) use vastly more power than traditional web servers. This requires larger backup generator capacities. In places like Northern Virginia (a global data center hub), data centers have sought permits to run diesel generators more frequently due to grid instability, leading to local air quality violations.


| Pollutant | Primary On-Site Source (Data Center) | Primary Off-Site Source (Grid) | Primary Health/Env Risk |
| :--- | :--- | :--- | :--- |
| **$NO_x$** | Diesel Backup Generators | Coal & Gas Power Plants | Smog formation, asthma, acid rain |
| **$SO_2$** | Diesel Fuel (impurities) | Coal Power Plants | Acid rain, respiratory irritation |
| **PM** | Diesel Exhaust (Soot) | Coal combustion ash | Lung damage, heart disease |

Because of the physics of these pollutants, they create "hotspots."
* **$CO_2$** disperses into the global atmosphere.
* **$NO_x$ and PM** hang in the air *locally*.
This means communities living next to "Hyperscale" AI data centers face a **direct health risk** from the exhaust of backup generators, distinct from the global climate change risk of the electricity usage.


## 5. Pipeline Emissions

### 5.1. Training and Fine Tuning Processes

The initial creation of a model is the most concentrated source of emissions. A seminal study from the University of Massachusetts, Amherst, found that training a single large AI model can emit over 626,000 pounds (284,000 kg) of carbon dioxide equivalent. Nearly five times the lifetime emissions of an average western cars.[82](https://www.technologyreview.com/2019/06/06/239031/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/) [83](https://news.climate.columbia.edu/2023/06/09/ais-growing-carbon-footprint/) [84](https://jpt.spe.org/training-single-ai-model-can-emit-much-carbon-five-cars-their-lifetimes) [85](https://news.mit.edu/2020/artificial-intelligence-ai-carbon-footprint-0423) [86](https://www.supermicro.com/en/article/ai-training-5-tips-reduce-environmental-impact) [87](https://medium.com/@rogt.x1997/ais-dirty-secret-how-gpt-3-consumed-1-287-mwh-and-emitted-the-same-co%E2%82%82-as-112-cars-5e43b85eb600) [88](https://www.embedl.com/knowledge/thecarbonfootprintofai.com) [89](https://icecat.com/is-ai-truly-a-sustainable-choice/) [90](https://carboncredits.com/how-big-is-the-co2-footprint-of-ai-models-chatgpts-emissions/) [91](https://projectexigence.eu/green-ict-digest/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/)


### 5.2 Inference Emissions

While training is a massive "one-time" cost, inference represents the ongoing energy cost every time the model is used. [91](https://www.clarifai.com/blog/training-vs-inference/) [92](https://nebius.com/blog/posts/difference-between-ai-training-and-inference) [93](https://www.finout.io/blog/the-new-economics-of-ai-balancing-training-costs-and-inference-spend#)

While a single inference pass uses a fraction of the energy of training, the aggregate volume is massive. For widely used models, inference emissions can surpass training emissions within weeks of deployment due to millions of daily queries. [94](https://www.sciencedirect.com/science/article/pii/S2542435123003653) [95](https://arxiv.org/html/2507.11417v1)

A standard older type keyword search (like non-ai legacy Google) was computationally cheap. A generative AI query requires massive matrix multiplication operations on GPUs, estimated to use 10x to 30x more energy than a traditional search query.
[96](https://medium.com/@clima10/sorry-mr-ai-but-were-out-of-power-the-looming-energy-crisis-for-generative-ai-at-scale-533934085375)
[97](https://www.reworked.co/knowledge-findability/environmental-concerns-may-push-companies-to-rethink-how-they-use-genai/)
[98](https://www.eidosmedia.com/updater/technology/AI-Shakes-Up-Search-and-SEO)
[99](https://one5c.com/google-search-ai-climate-impact-136965951/)

### 5.3. Agentic Emissions

This is a rapidly growing and critically inefficient sector of AI emissions. Unlike a standard chatbot that answers a question once, an AI Agent is designed to operate autonomously to achieve a goal. 
Agentic Emissions represent a recurring, compounding operational cost that scales linearly (or exponentially) with utility.

So there is a base-increase of the computational spending from the start. It's not a single query at a time but a whole pipeline generating much more heating, therefore, in need of even stronger cooling systems that usually consume even more water. 

A single user prompt to an agent does not result in one model inference. It triggers a "Chain of Thought" or a loop where the agent talks to itself, breaks the task into steps, queries the web, checks its work, and corrects errors. One user goal can trigger hundreds or thousands of API calls behind the scenes.

In Standard LLMs it's one token in, probability distribution out. The compute cost is relatively deterministic based on the token count.

In Agentic Workflows the "prompt" is merely the starting gun. The compute cost is non-deterministic because the agent decides how much compute it needs to solve the problem.

sophisticated agents use "Reflection" steps. They generate code, then a second instance of the model critiques that code, and a third instance refines it.

The context window (memory) grows with every step, making every subsequent inference computationally heavier than the last.


##### 5.3.1 The "Loop of Doom" 

Agents frequently get stuck. An agent might attempt to write code or browse a website, fail, analyze the error, and retry indefinitely.

An agent could run on high-performance GPUs for hours, consuming kilowatt-hours of electricity, only to fail the task.

An agent trying to debug a Python script may get stuck in a syntax error loop. It will run the code, see the error, try to fix it, introduce a new error, and repeat.

If this runs on an H100 GPU cluster (drawing ~700W per chip), a 30-minute "Loop of Doom" consumes significant electricity with zero economic output. This is "Zombie Compute"—energy burned for dead processes.

The physical consequence of the described current software pipeline  is heat.

A chatbot server might have spikes of usage. An agent running a complex task keeps the GPUs pinned at 100% utilization for extended periods.

The longer the agent "thinks" (loops), the more water is evaporated to keep the silicon from melting.

For every kWh of energy consumed by the agent, a specific amount of water (often liters) is consumed.

To prevent Agentic Emissions from becoming an environmental disaster, the industry needs to be looking toward several efficiency levers, like the ones we gonna talk about it here. 


#### 5.3.2 Agentic Water Usage and Justifiability:

As Data centers generate this immense heat; to keep the GPUs from melting, they end up consuming billions of liters of water anually for evaporative cooling because all companies worlwide are doing those processes in parallel. [100](https://theconversation.com/data-centers-consume-massive-amounts-of-water-companies-rarely-tell-the-public-exactly-how-much-262901) [101](https://www.digitalrealty.com/resources/articles/future-of-data-center-cooling) [102](https://www.bloomberg.com/graphics/2025-ai-impacts-data-centers-water-data/) [103](https://www.aquatechtrade.com/news/digital-solutions/ai-water-usage) [104](https://mateussaldanha.substack.com/p/cooling-ai-data-centers-and-in-an) [105](https://ethicalgeo.org/the-cloud-is-drying-our-rivers-water-usage-of-ai-data-centers/) [106](https://www.sangfor.com/blog/cloud-and-infrastructure/data-center-cooling-systems-challenges-and-solutions) [107](https://lifestyle.sustainability-directory.com/learn/what-are-alternative-cooling-technologies-for-data-centers-that-reduce-water-usage) [108](https://www.iceotope.com/learning-hub/insights/an-introduction-to-data-center-cooling/) [109](https://lifestyle.sustainability-directory.com/learn/what-are-alternative-cooling-technologies-for-data-centers-that-save-water/) [110](https://www.google.com/search?q=https://www.vertiv.com/en-us/about/news-and-insights/articles/educational-articles/understanding-direct-to-chip-cooling-in-hpc-infrastructure-a-deep-dive-into-liquid-cooling)

One Google data center in Iowa consumed 3.8 billion liters of water in one year. [111](https://www.bloomberg.com/graphics/2025-ai-impacts-data-centers-water-data/) [112](https://theconversation.com/data-centers-consume-massive-amounts-of-water-companies-rarely-tell-the-public-exactly-how-much-262901) [113](https://www.bloomberg.com/graphics/2025-ai-impacts-data-centers-water-data/)

For an agent that spins in circles for hours without success, the water footprint becomes unjustifiably high. We are effectively evaporating potable water and burning fossil fuels for a software process that yields zero economic or utility value.

### 5.4 Memory Systems Emissions

Retrieval-Augmented Generation (RAG) and long-term memory allow AI to "remember" data, but this adds a distinct enviromental impact layer. [114](https://www.f22labs.com/blogs/what-is-retrieval-augmented-generation-rag/)

The physical hard drives or solid-state drives (SSDs) used for long-term memory have a lifecycle impact, from raw material extraction to manufacturing, usage power consumption, and eventual disposal or recycling [115](https://blog.loop.homes/the-environmental-impact-of-chatgpt)

While RAG can reduce the need for larger, more expensive core models, the added steps of indexing data and performing lookups mean that the total operational computation per user query can be higher than simple, self-contained inference. [116](https://tensorwave.com/glossary/retrieval-augmented-generation-rag) [117](https://www.tapclicks.com/blog/retrieval-augmented-generation-rag-what-is-it-and-its-benefits-for-your-business) [118](https://www.evolvingdev.com/post/using-retrieval-augmented-generation-rag-in-artificial-intelligence) [119](https://www.codecademy.com/article/retrieval-augmented-generation-in-ai) [120](https://wandb.ai/onlineinference/genai-research/reports/Tutorial-MUVERA-Weights-Biases-Fast-scalable-multi-vector-retrieval--VmlldzoxMzY5MTUwOA)

To remember information, data must be converted into "embeddings" (mathematical vectors) and stored in high-performance databases. These databases require additional constant power to keep data indexed and ready for retrieval (gh-performance vector databases typically use significant RAM for fast indexing and retrieval). [121](https://medium.com/@sateeshfrnd/vector-databases-storing-and-retrieving-ai-memory-efficiently-91a2d9542fd7) [122](https://qdrant.tech/articles/what-is-a-vector-database) 
[123](https://www.designveloper.com/blog/what-is-vector-database) [123](https://medium.com/@dresraceran/indexing-memory-and-database-0c402c3c394a)

The number of vectors and their dimensionality are the primary drivers of memory usage. A naive 1024-dimensional float vector can require substantial RAM for millions or billions of vectors

Additional data associated with vectors, such as payload fields used for filtering, also contribute to memory usage. [124](https://milvus.io/ai-quick-reference/how-does-vector-search-manage-memory-usage)
[125](https://bhargavaparv.medium.com/managing-millions-of-high-dimensional-vectors-in-modern-vector-database-cbad318068fe) [126](https://qdrant.tech/documentation/guides/capacity-planning/)

As knowledge bases change, the models must constantly re-read and re-embed documents. Continuously updating the "memory" of an AI system creates a baseline energy load that exists even when no users are querying the system.

Training data (petabytes of text and video) must be stored on servers that run 24/7. These servers are replicated across multiple geographies for redundancy, meaning the energy cost of storing the dataset is multiplied by 2x or 3x.

Moving petabytes of data between data centers for training or fine-tuning generates significant heat and requires extensive networking gear (switches, routers, fiber optics), all of which have their own energy footprints.


### 5.5 Rare earth metals 

The manufacturing of H100 or A100 GPUs requires mining rare earth metals and ultra-precise silicon lithography. This process is incredibly carbon-intensive. If an AI system requires new hardware upgrades every 18 months, the embodied carbon of the physical chips often outweighs the electricity used to run them.

Over 70% of the world's cobalt comes from the Democratic Republic of Congo (DRC). [127](https://www.facebook.com/channelsforum/posts/the-drc-produces-more-than-70-percent-of-the-worlds-supply-of-cobalt-which-is-es/1493781458776267) [128](https://natural-resources.canada.ca/minerals-mining/mining-data-statistics-analysis/minerals-metals-facts/cobalt-facts)

The neoliberalist dfinition of 'green energy transition' was one of the last hopes of keeping our home planet from imploding.

While the DRC is home to some of the richest mineral deposits in the world, supplying around 70 percent of the world’s cobalt, its people have remained deeply impoverished. [129](https://farmonaut.com/mining/cobalt-siddharth-kara-7-ethical-issues-in-drc-mining)

entire communities are often exposed to dangerous working conditions, toxic pollution and violence. Some areas are under the control of armed groups, leaving locals without the basic right to live and work safely. [130](https://thinklandscape.globallandscapesforum.org/73584/cobalt-mining-dr-congo-green-transition/)

This has led to DRC being described as a green sacrifice zone, a place exploited in the name of sustainability. [131](https://intpolicydigest.org/your-smart-device-is-powered-by-child-labour/) [132](https://www.chemistryworld.com/news/congos-cobalt-conundrum/4021696.article) [133](https://www.savethechildren.net/stories/drc-cobalt-mines-child-labour-and-green-transition)

Children in mining areas in the Democratic Republic of Congo (DRC) expressed in artistic forms to their communities the child labour-free future they want. [134](https://www.ilo.org/resource/news/tiny-mighty-voices-against-child-labour-cobalt-mining)

There are approximately 110,000 to 150,000 artisanal miners in this region, who work alongside much larger industrial operations. These artisanal miners, referred to as creuseurs in the DRC, mine by hand using the most basic tools to dig out rocks from tunnels deep underground. [135](https://www.amnesty.org/fr/wp-content/uploads/2021/05/AFR6231832016ENGLISH.pdf) [136](https://borgenproject.org/human-rights-abuses-in-the-drc/)

Artisanal miners include children as young as seven who scavenge for rocks containing cobalt in the discarded by-products of industrial mines, and who wash and sort the ore before it is sold. [137](https://www.amnesty.org/fr/wp-content/uploads/2021/05/AFR6231832016ENGLISH.pdf) [138](https://www.dol.gov/agencies/ilab/reports/child-labor/list-of-goods/supply-chains/lithium-ion-batteries) [139](https://www.cfr.org/blog/why-cobalt-mining-drc-needs-urgent-attention) [140](https://www.businessinsider.com/photos-terrible-conditions-cobalt-mining-industry-to-meet-battery-demands-2023-2) [141](https://farmonaut.com/mining/blood-cobalt-mining-in-drc-7-urgent-ethical-challenges-2025) [142](https://adf-magazine.com/2023/10/chinese-mining-wrecking-lives-in-drc/)

Chronic exposure to dust containing cobalt can result in a potentially fatal lung disease, called “hard metal lung disease.” Inhalation of cobalt particles can also cause “respiratory sensitization, asthma, shortness of breath, and decreased pulmonary function”, and sustained skin contact with cobalt can lead to dermatitis. Yet researchers found that the vast majority of miners, who spend long hours every day working with cobalt, do not have the most basic of protective equipment, such as gloves, work clothes or facemasks. [143](https://www.amnesty.org/fr/wp-content/uploads/2021/05/AFR6231832016ENGLISH.pdf) [144](https://www.wilsoncenter.org/blog-post/drc-mining-industry-child-labor-and-formalization-small-scale-mining) [145](https://www.amnesty.org/en/latest/news/2023/09/drc-cobalt-and-copper-mining-for-batteries-leading-to-human-rights-abuses/) [146](https://www.wbur.org/onpoint/2024/03/13/human-cost-cobalt-modern-slavery-in-the-democratic-republic-of-congo) [147](https://www.npr.org/sections/goatsandsoda/2023/02/01/1152893248/red-cobalt-congo-drc-mining-siddharth-kara) [148](https://abcnews.go.com/International/cobalt-mining-transforms-city-democratic-republic-congo-satellite/story?id=96795773) [149](https://www.theguardian.com/global-development/2021/nov/08/cobalt-drc-miners-toil-for-30p-an-hour-to-fuel-electric-cars) 


### 5.6 Teacher Models

There's also a hidden or externalized costs that are often excluded from standard energy efficiency metrics, which tend to focus only on the final model's inference (use) or training (initial development).

The full energy footprint of a deployed, resource-equilibrated AI model includes several computationally expensive phases.

The initial Superior Model Training, the massive training run of the largest possible "Teacher" model, often conducted in highly secure, isolated (air-gapped) data centers.

The "superior model" is then used to generate a vast amount of high-quality synthetic data, the "content", which serves as the training dataset for the smaller model.

This is known as inference at scale on the teacher model. While inference is less power-intensive than training, performing it for billions of data points to create a distillation dataset adds substantial, often unquantified, operational energy usage.

Knowledge Distillation is the process of training a smaller, faster "Student" model using the outputs (the "knowledge") of the large "Teacher" model.

Even though the student model is smaller and more efficient for final deployment, the distillation process itself is a significant training run that requires substantial energy to efficiently transfer the knowledge.


### 5.7 Errors and bugged runs

Failures in training and auxiliary system processes, caused by issues like package conflicts, data corruption, or poor model fit (under/overfitting), lead to a severely additional resource consumption.


### 5.8 Technical benchmarks

Many niche models possess unique value but are discarded because they fail to top general technical benchmarks. Researchers often evaluate dozens of models rapidly; if a model does not impress immediately, sometimes due merely to faulty inference code rather than the model itself, it is permanently set aside. This premature abandonment represents a significant sunk cost, rendering the substantial water consumption and carbon emissions expended during training completely wasted.

This turns the entire training process into an environmental tragedy, wasting the vast amounts of energy and water used to create a tool that no one will ever use.

The core problem lies in 'unconnected' technical benchmarks. These metrics are so hyper-specific that they create a vacuum, effectively stripping away systems thinking. They evaluate models as if they exist in isolation, ignoring the crucial reality that AI is woven into the fabric of society and physically dependent on our planetary biomes. By optimizing for a narrow score, we ignore the holistic cost of the system. Standard benchmarks fail because they lack a holistic view, as they ignore how the pipelines extract from our biomes and impacts the social and enviromental fabric.

There are thousands of distinct physical locations (Data Centers) that compete with local agriculture and residents for water tables and energy grids.

A benchmark might say a model is "State of the Art" (SOTA), but if that SOTA status requires 3x the energy for 1% better reasoning, a holistic benchmark, like eco-benchmark, would rate it as a failure.

models that may be niche great but never reach technical benchmarks, not even being used if maybe didnt impress enough in the first impression of a researcher that tests many models. maybe it was not even something about the model but in the infernece code, still, another resource-consumption wasted, with the water consumption and emissions described ehere.


---

Ronni Ross  
2025
